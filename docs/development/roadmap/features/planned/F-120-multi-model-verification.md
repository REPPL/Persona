# F-120: Multi-Model Verification

## Overview

| Attribute | Value |
|-----------|-------|
| **Use Case** | UC-004, UC-007 |
| **Milestone** | v1.7.0 |
| **Priority** | P2 |
| **Category** | Quality |
| **Status** | Planned |

## Problem Statement

LLM outputs are inherently non-deterministic. A persona generated by one model may contain hallucinations or biases specific to that model's training data. Without cross-model verification, users cannot distinguish between genuine insights and model-specific artifacts. Research shows that hallucinated outputs are not reproducible across independent samples.

## Research Foundation

### Academic Sources

- **SelfCheckGPT**: Sampling-based fact-checking assuming hallucinations aren't reproducible
- **HELM Benchmark**: Holistic evaluation standards for cross-model comparison
- **Nature Machine Intelligence (2024)**: Dataset provenance auditing across models

### Key Findings

- If an LLM has knowledge of a concept, sampled responses contain consistent facts
- Ensemble checks (re-querying under multiple settings and voting) improve reliability
- Cross-vendor comparison requires standardised evaluation conditions
- No major LLM delivers fully consistent multi-constraint outputs without human review

## Design Approach

### Verification Methods

| Method | Description | Use Case |
|--------|-------------|----------|
| **Self-Consistency** | Multiple samples from same model | Quick sanity check |
| **Cross-Model** | Same prompt to different models | Identify model-specific artifacts |
| **Ensemble Voting** | Majority agreement across models | High-confidence validation |

### Architecture

```
Source Data + Prompt
        │
        ▼
┌───────────────────┐
│  Model Dispatcher │
└───────┬───────────┘
        │
   ┌────┼────┬────┐
   │    │    │    │
   ▼    ▼    ▼    ▼
Model  Model Model Model
  A      B     C     D
   │    │    │    │
   └────┴────┴────┘
        │
        ▼
┌───────────────────┐
│ Consistency Check │
│  - Attribute match│
│  - Semantic sim   │
│  - Voting         │
└───────┬───────────┘
        │
        ▼
   VerificationReport
```

### Consistency Metrics

| Metric | Description | Range |
|--------|-------------|-------|
| **Attribute Agreement** | % of attributes present in all outputs | 0.0 - 1.0 |
| **Semantic Consistency** | Embedding similarity across outputs | 0.0 - 1.0 |
| **Factual Convergence** | Claims appearing in majority of outputs | 0.0 - 1.0 |
| **Confidence Score** | Weighted combination of above | 0.0 - 1.0 |

### Python API

```python
from persona.core.quality.verification import (
    MultiModelVerifier,
    VerificationConfig,
)

# Configure verification
config = VerificationConfig(
    models=["claude-sonnet-4-20250514", "gpt-4o", "gemini-2.0-flash"],
    samples_per_model=3,
    consistency_threshold=0.7,
    voting_strategy="majority",  # or "unanimous", "weighted"
)

# Create verifier
verifier = MultiModelVerifier(config)

# Verify persona generation
result = await verifier.verify(
    source_data=data,
    prompt_template="generate_persona",
    count=3,
)

# Check results
print(f"Consistency score: {result.consistency_score}")
print(f"High-confidence attributes: {result.agreed_attributes}")
print(f"Disputed attributes: {result.disputed_attributes}")

# Get consensus persona (attributes agreed by majority)
consensus = result.get_consensus_persona()
```

### CLI Interface

```bash
# Verify with multiple models
persona verify output/personas.json \
    --models claude-sonnet-4-20250514,gpt-4o,gemini-2.0-flash \
    --samples 3

# Self-consistency check (same model, multiple samples)
persona verify output/personas.json --self-consistency --samples 5

# Generate verification report
persona verify output/ --report verification-report.md

# Regenerate with cross-model consensus
persona generate data.csv --verify --models claude-sonnet-4-20250514,gpt-4o --consensus
```

## Implementation Tasks

- [ ] Create verification module structure (`persona/core/quality/verification/`)
- [ ] Implement MultiModelVerifier class
- [ ] Add self-consistency checking (single model, multiple samples)
- [ ] Add cross-model verification with parallel generation
- [ ] Implement attribute-level agreement calculation
- [ ] Add semantic similarity comparison using embeddings
- [ ] Implement voting strategies (majority, unanimous, weighted)
- [ ] Create VerificationReport model with detailed breakdowns
- [ ] Add consensus persona extraction
- [ ] Add `persona verify` CLI command
- [ ] Add `--verify` flag to `persona generate`
- [ ] Write unit tests
- [ ] Write integration tests with mock providers
- [ ] Add documentation

## Success Criteria

- [ ] Self-consistency check detects inconsistent outputs
- [ ] Cross-model verification identifies model-specific content
- [ ] Consensus extraction produces reliable personas
- [ ] Voting strategies configurable and documented
- [ ] CLI integration seamless
- [ ] Test coverage ≥ 85%
- [ ] Verification adds < 3x generation time (parallel execution)

## Dependencies

- F-002: LLM Provider Abstraction (multi-provider support)
- F-066: Multi-Model Generation (parallel execution)
- F-088: Async Support (concurrent API calls)

## Risks & Mitigations

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| API cost multiplication | High | Medium | Configurable model count, caching |
| Rate limiting across providers | Medium | Medium | Configurable delays, retry logic |
| Models may share training biases | Medium | Low | Document limitations |
| Semantic similarity false positives | Low | Low | Configurable thresholds |

---

## Related Documentation

- [Milestone v1.7.0](../../milestones/v1.7.0.md)
- [F-066: Multi-Model Generation](../completed/F-066-multi-model-generation.md)
- [Provider APIs Reference](../../../../reference/provider-apis.md)

---

**Status**: Planned
