# Milestone: v1.3.0 - Local Model Foundation

Local LLM support and privacy-preserving data handling.

## Goal

Enable fully local persona generation with Ollama and provide enterprise-grade privacy protection through PII detection and anonymisation.

## Theme

**"Local & Private"** - Full local operation with privacy guarantees.

## Features (0 of 2 Complete)

| ID | Feature | Category | Status |
|----|---------|----------|--------|
| [F-112](../features/planned/F-112-native-ollama-provider.md) | Native Ollama Provider | Provider | ðŸ“‹ Planned |
| [F-113](../features/planned/F-113-pii-detection-anonymisation.md) | PII Detection & Anonymisation | Privacy | ðŸ“‹ Planned |

## Success Criteria

- [ ] `persona generate --provider ollama` works with local models
- [ ] Auto-detection of available Ollama models
- [ ] `persona generate --anonymise` removes PII from input data
- [ ] Three anonymisation strategies: redact, replace, hash
- [ ] `persona privacy scan` previews detected PII
- [ ] Works offline (no cloud API calls required)
- [ ] Test coverage >= 90%

## Use Cases Addressed

| Use Case | How Addressed |
|----------|---------------|
| Air-gapped environments | Full Ollama support, no internet required |
| GDPR compliance | PII anonymisation before processing |
| Cost-conscious users | Free local generation with 70B models |
| Sensitive research data | Data stays on-premise |

## Installation

```bash
# Install with privacy support
pip install persona[privacy]

# Generate with local model
persona generate --provider ollama --model qwen2.5:72b

# Anonymise sensitive data
persona generate --input sensitive.csv --anonymise
```

## Dependencies

- v1.2.0: Extensibility & TUI (plugin system for provider registration)
- Ollama installed locally with at least one model
- Optional: spaCy model for PII detection (`en_core_web_lg`)

## Non-Goals

- Custom model fine-tuning (use existing models)
- Remote Ollama servers (local only for now)
- Real-time PII detection in TUI (batch only)

## Technical Considerations

### Ollama Integration

```python
# Provider registration via plugin system
from persona.core.providers import Provider

class OllamaProvider(Provider):
    def __init__(self, model: str = "qwen2.5:72b"):
        self.model = model
        self.client = OllamaClient()

    def generate(self, prompt: str) -> str:
        return self.client.chat(model=self.model, messages=[...])
```

### PII Pipeline

```
Input Data â†’ presidio-analyzer â†’ Entity Detection â†’ Anonymisation Strategy â†’ Safe Output
                    â†“                    â†“                    â†“
               spaCy NER         PERSON, EMAIL,        redact | replace | hash
                                 PHONE, ADDRESS
```

## Risks & Mitigations

| Risk | Mitigation |
|------|------------|
| Ollama not installed | Clear error message with install instructions |
| Model too slow | Recommend hardware requirements, show progress |
| PII false negatives | Conservative detection, document limitations |
| Large dependency size | Optional extras, not in core package |

---

## Related Documentation

- [Milestones Overview](README.md)
- [v1.2.0: Extensibility & TUI](v1.2.0.md) (prerequisite)
- [v1.4.0: Quality & Data Generation](v1.4.0.md) (next)
- [F-112: Native Ollama Provider](../features/planned/F-112-native-ollama-provider.md)
- [F-113: PII Detection & Anonymisation](../features/planned/F-113-pii-detection-anonymisation.md)
- [R-013: Local Model Assessment](../../research/R-013-local-model-assessment.md)

---

**Status**: Planned
