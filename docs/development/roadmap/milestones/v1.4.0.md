# Milestone: v1.4.0 - Quality & Data Generation

Local LLM evaluation and synthetic data generation for privacy-preserving workflows.

## Goal

Provide automated quality evaluation using local LLM-as-judge and enable synthetic data generation to replace sensitive datasets while preserving statistical properties.

## Theme

**"Evaluate & Generate"** - Quality assurance and data transformation.

## Features (2 of 2 Complete)

| ID | Feature | Category | Status |
|----|---------|----------|--------|
| [F-114](../features/completed/F-114-llm-as-judge-evaluation.md) | LLM-as-Judge Persona Evaluation | Quality | ✅ Complete |
| [F-115](../features/completed/F-115-synthetic-data-generation.md) | Synthetic Data Generation Pipeline | Privacy | ✅ Complete |

## Success Criteria

- [ ] `persona evaluate` scores personas on coherence, realism, usefulness
- [ ] Local LLM evaluation costs nothing (Ollama-based)
- [ ] Evaluation scores correlate >70% with human judgement
- [ ] `persona synthesise` generates privacy-safe synthetic data
- [ ] Synthetic data preserves >85% distribution similarity
- [ ] Zero PII in synthetic output (verified by F-113)
- [ ] Test coverage >= 90%

## Use Cases Addressed

| Use Case | How Addressed |
|----------|---------------|
| Quality assurance | Automated evaluation without manual review |
| Sensitive training data | Synthetic data generation preserves utility |
| Bulk evaluation | Batch processing with local models |
| Research validation | Objective quality metrics for persona sets |

## Installation

```bash
# Evaluate persona quality
persona evaluate personas.json --judge ollama

# Generate synthetic data from sensitive source
persona synthesise --input interviews.csv --output synthetic.csv
```

## Dependencies

- v1.3.0: Local Model Foundation (Ollama provider, PII detection)
- F-106: Quality Metrics Scoring (existing scoring framework)

## Non-Goals

- Human-in-the-loop evaluation (automated only)
- Real-time streaming evaluation (batch processing)
- Custom evaluation criteria (use predefined metrics)

## Technical Considerations

### LLM-as-Judge Architecture

```python
from persona.core.evaluation import PersonaJudge

judge = PersonaJudge(provider="ollama", model="llama3:70b")

scores = judge.evaluate_batch(personas, criteria=[
    EvaluationCriteria.COHERENCE,
    EvaluationCriteria.REALISM,
    EvaluationCriteria.DISTINCTIVENESS,
])
```

### Synthetic Data Pipeline

```
Original Data → Statistical Analysis → Distribution Extraction → LLM Generation → Validation
                       ↓                       ↓                      ↓              ↓
                  Schema +              Frequencies +           Synthetic       PII Check +
                  Types                   Ranges                Records         Quality
```

### Evaluation Criteria

| Criterion | Description | Scoring |
|-----------|-------------|---------|
| Coherence | Internal consistency of attributes | 0.0 - 1.0 |
| Realism | Believability as a real person | 0.0 - 1.0 |
| Usefulness | Value for design decisions | 0.0 - 1.0 |
| Distinctiveness | Uniqueness within persona set | 0.0 - 1.0 |

## Risks & Mitigations

| Risk | Mitigation |
|------|------------|
| LLM evaluation bias | Document limitations, compare to human scores |
| Inconsistent scores | Low temperature, multiple evaluation passes |
| Synthetic data quality | Statistical validation, iterative refinement |
| PII leakage | Multi-pass scanning, conservative detection |

---

## Related Documentation

- [Milestones Overview](README.md)
- [v1.3.0: Local Model Foundation](v1.3.0.md) (prerequisite)
- [v1.5.0: Hybrid Pipeline](v1.5.0.md) (next)
- [F-114: LLM-as-Judge Persona Evaluation](../features/completed/F-114-llm-as-judge-evaluation.md)
- [F-115: Synthetic Data Generation Pipeline](../features/completed/F-115-synthetic-data-generation.md)
- [F-106: Quality Metrics Scoring](../features/completed/F-106-quality-metrics.md)
- [R-013: Local Model Assessment](../../research/R-013-local-model-assessment.md)

---

**Status**: Complete
