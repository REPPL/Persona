# Manual Test Script: v0.9.0 - Logging & Monitoring

This script verifies all v0.9.0 functionality. Execute tests in order and record results.

## Prerequisites

- Python 3.12+
- Git checkout of v0.9.0 tag
- No API keys required (tests use mock responses)

## Installation

```bash
# 1. Checkout the version
git checkout v0.9.0

# 2. Create fresh virtual environment
python3.12 -m venv .venv-test
source .venv-test/bin/activate  # On Windows: .venv-test\Scripts\activate

# 3. Install Persona with all dependencies
pip install -e ".[all]"

# 4. Verify installation
python -c "import persona; print(f'Persona v{persona.__version__}')"
```

**Expected:** Shows Persona v0.9.0

---

## Test 1: Module Import Test

**Command:**
```bash
python -c "from persona.core.logging import ExperimentLogger, StructuredLogger, ProgressTracker; print('Logging modules imported successfully')"
```

**Expected:** Prints "Logging modules imported successfully"

**Pass Criteria:**
- [ ] Import completes without error
- [ ] Success message displayed

---

## Test 2: Experiment Logger (F-073)

**Command:**
```bash
python -c "
from persona.core.logging import ExperimentLogger, EventType, LogLevel

logger = ExperimentLogger(experiment_id='exp-test-001')

# Log various events
logger.info(EventType.EXPERIMENT_STARTED, {'model': 'claude'}, 'Starting experiment')
logger.debug(EventType.DATA_LOADED, {'files': 3})
logger.info(EventType.GENERATION_COMPLETED, {'personas': 5})

# Get events
events = logger.get_events()
print(f'Events logged: {logger.event_count}')
print(f'Info events: {len(logger.get_events(level=LogLevel.INFO))}')
print('ExperimentLogger: OK')
"
```

**Expected:** Shows 3 events logged, 2 info events, then "ExperimentLogger: OK"

**Pass Criteria:**
- [ ] ExperimentLogger creates successfully
- [ ] Events are logged correctly
- [ ] Event filtering by level works

---

## Test 3: JSON Lines File Logging

**Command:**
```bash
python -c "
import tempfile
from pathlib import Path
from persona.core.logging import ExperimentLogger, EventType, read_log_file

with tempfile.TemporaryDirectory() as tmpdir:
    log_dir = Path(tmpdir) / 'logs'

    with ExperimentLogger(experiment_id='exp-file-test', log_dir=log_dir) as logger:
        logger.info(EventType.EXPERIMENT_STARTED)
        logger.info(EventType.DATA_LOADED, {'count': 5})
        logger.info(EventType.EXPERIMENT_COMPLETED)

    # Read back the log file
    log_file = log_dir / 'experiment.jsonl'
    events = read_log_file(log_file)
    print(f'Events in file: {len(events)}')
    print(f'First event type: {events[0].event.value}')
    print('JSON Lines logging: OK')
"
```

**Expected:** Shows 3 events in file, first event is EXPERIMENT_STARTED, then "JSON Lines logging: OK"

**Pass Criteria:**
- [ ] Log file created in specified directory
- [ ] Events can be read back from file
- [ ] JSON Lines format is valid

---

## Test 4: Structured Logging (F-074)

**Command:**
```bash
python -c "
from persona.core.logging import StructuredLogger, LogContext, OutputFormat

# Create logger with context
logger = StructuredLogger(output_format=OutputFormat.JSON)
bound = logger.bind(experiment_id='exp-123', run_id='run-456')

# Log events
entry = bound.info('data_loaded', files=3, tokens=5000)
bound.warn('rate_limit_approaching', remaining=5)
bound.error('api_error', status=500)

entries = logger.get_entries()
print(f'Entries logged: {len(entries)}')
print(f'Context preserved: {entries[0].context.experiment_id}')
print('StructuredLogger: OK')
"
```

**Expected:** Shows 3 entries logged, context "exp-123" preserved, then "StructuredLogger: OK"

**Pass Criteria:**
- [ ] StructuredLogger creates successfully
- [ ] Context binding works
- [ ] Entries retrievable

---

## Test 5: Progress Tracking (F-075)

**Command:**
```bash
python -c "
from persona.core.logging import ProgressTracker

with ProgressTracker(title='Test Progress', quiet=True) as tracker:
    # Add main task
    task_id = tracker.add_task('Loading data', total=100)

    # Update progress
    tracker.update(task_id, advance=30)
    tracker.update(task_id, advance=40)
    tracker.update(task_id, advance=30)

    # Complete
    tracker.complete(task_id)

    task = tracker.get_task(task_id)
    print(f'Task status: {task.status}')
    print(f'Completed: {task.percent:.0f}%')

# Test subtasks
with ProgressTracker(quiet=True) as tracker:
    parent_id = tracker.add_task('Main task', total=100)

    with tracker.subtask(parent_id, 'Subtask 1', total=50) as sub_id:
        tracker.update(sub_id, advance=50)

    parent = tracker.get_task(parent_id)
    print(f'Subtasks: {len(parent.subtasks)}')
    print(f'Subtask status: {parent.subtasks[0].status}')

print('ProgressTracker: OK')
"
```

**Expected:** Shows task completed at 100%, subtask handling works, then "ProgressTracker: OK"

**Pass Criteria:**
- [ ] ProgressTracker creates successfully
- [ ] Task progress updates correctly
- [ ] Subtask support works
- [ ] Quiet mode suppresses output

---

## Test 6: Metadata Recording (F-076)

**Command:**
```bash
python -c "
from persona.core.logging import MetadataRecorder, calculate_checksum

# Create recorder
recorder = MetadataRecorder(experiment_id='exp-meta-test', run_id='run-001')

# Record generation
recorder.start()
recorder.set_config(model='claude-sonnet-4', provider='anthropic', persona_count=3)
recorder.add_data_source('interview.csv', tokens=5000, size_bytes=10240)
recorder.set_costs(input_tokens=10000, output_tokens=2000, total_cost_usd=0.15)

# Finish and get metadata
metadata = recorder.finish()

print(f'Experiment: {metadata.experiment_id}')
print(f'Model: {metadata.configuration.model}')
print(f'Duration: {metadata.duration_seconds:.2f}s')
print(f'Cost per persona: \${metadata.costs.cost_per_persona:.4f}')

# Test checksum
checksum = calculate_checksum('test content')
print(f'Checksum prefix: {checksum[:12]}...')

print('MetadataRecorder: OK')
"
```

**Expected:** Shows experiment ID, model, duration, cost per persona, checksum, then "MetadataRecorder: OK"

**Pass Criteria:**
- [ ] MetadataRecorder creates successfully
- [ ] Timing captured correctly
- [ ] Cost per persona calculated
- [ ] Checksum function works

---

## Test 7: Token Usage Logging (F-077)

**Command:**
```bash
python -c "
from persona.core.logging import TokenUsageLogger, TokenBreakdown

logger = TokenUsageLogger(run_id='run-token-test')

# Log multiple calls
logger.log(
    step='generation',
    model='claude-sonnet-4',
    provider='anthropic',
    input_tokens=10000,
    output_tokens=2000,
    cost_usd=0.15,
    breakdown=TokenBreakdown(system=1000, data=7000, instructions=2000),
)

logger.log(
    step='validation',
    model='claude-sonnet-4',
    provider='anthropic',
    input_tokens=5000,
    output_tokens=500,
    cost_usd=0.05,
)

# Get summary
summary = logger.get_summary()
print(f'Total calls: {summary.call_count}')
print(f'Total tokens: {summary.total_tokens:,}')
print(f'Total cost: \${summary.total_cost_usd:.2f}')
print(f'Output/Input ratio: {summary.output_input_ratio:.1%}')

# Export formats
jsonl = logger.to_jsonl()
csv = logger.to_csv()
print(f'JSONL lines: {len(jsonl.strip().split(chr(10)))}')
print(f'CSV lines: {len(csv.strip().split(chr(10)))}')

print('TokenUsageLogger: OK')
"
```

**Expected:** Shows 2 calls, ~17,500 tokens, $0.20 cost, export formats work, then "TokenUsageLogger: OK"

**Pass Criteria:**
- [ ] Token usage tracked correctly
- [ ] Summary calculations correct
- [ ] JSONL export works
- [ ] CSV export works

---

## Test 8: Cost Tracking (F-078)

**Command:**
```bash
python -c "
from persona.core.logging import CostTracker, BudgetConfig

# Create tracker with budget
budget = BudgetConfig(daily=10.00, warn_threshold=0.8, block_threshold=1.0)
tracker = CostTracker(budget=budget)

# Record costs
tracker.record(
    experiment_id='exp-cost-test',
    run_id='run-001',
    estimated=0.50,
    actual=0.48,
    model='claude-sonnet-4',
    input_tokens=10000,
    output_tokens=2000,
)

tracker.record(
    experiment_id='exp-cost-test',
    run_id='run-002',
    estimated=0.50,
    actual=0.52,
    model='claude-sonnet-4',
    input_tokens=10000,
    output_tokens=2500,
)

# Get summary
summary = tracker.get_summary(experiment_id='exp-cost-test')
print(f'Total runs: {summary.total_runs}')
print(f'Total actual: \${summary.total_actual:.2f}')
print(f'Variance: \${summary.total_variance:.4f}')

# Check budget
statuses = tracker.check_budget()
for status in statuses:
    print(f'{status.period}: {status.percent_used:.1f}% used ({status.status})')

print(f'Should block: {tracker.should_block()}')
print(f'Should warn: {tracker.should_warn()}')

print('CostTracker: OK')
"
```

**Expected:** Shows 2 runs, ~$1.00 total, budget status, should_block/warn indicators, then "CostTracker: OK"

**Pass Criteria:**
- [ ] Cost recording works
- [ ] Summary calculations correct
- [ ] Budget status checking works
- [ ] Warning/blocking thresholds work

---

## Test 9: Full Integration Test

**Command:**
```bash
python -c "
from persona.core.logging import (
    ExperimentLogger,
    StructuredLogger,
    ProgressTracker,
    MetadataRecorder,
    TokenUsageLogger,
    CostTracker,
    BudgetConfig,
    EventType,
    OutputFormat,
    calculate_checksum,
)

print('=== Full Logging Integration Test ===')
print()

# 1. Setup experiment logging
exp_logger = ExperimentLogger(experiment_id='exp-integration')
exp_logger.info(EventType.EXPERIMENT_STARTED, {'model': 'claude'})
print('1. Experiment logger initialised')

# 2. Setup structured logging
struct_logger = StructuredLogger(output_format=OutputFormat.CONSOLE)
bound = struct_logger.bind(experiment_id='exp-integration')
bound.info('integration_test_started')
print('2. Structured logger with context')

# 3. Track progress
with ProgressTracker(title='Integration Test', quiet=True) as tracker:
    task_id = tracker.add_task('Processing', total=100)
    tracker.update(task_id, completed=100)
    tracker.complete(task_id)
print('3. Progress tracking complete')

# 4. Record metadata
recorder = MetadataRecorder(experiment_id='exp-integration')
recorder.start()
recorder.set_config(model='claude', persona_count=3)
recorder.add_data_source('test.csv', tokens=5000)
recorder.set_costs(input_tokens=5000, output_tokens=1000, total_cost_usd=0.08)
checksum = calculate_checksum('test output')
recorder.add_checksum('output.json', checksum)
metadata = recorder.finish()
print(f'4. Metadata recorded (duration: {metadata.duration_seconds:.2f}s)')

# 5. Log token usage
token_logger = TokenUsageLogger(experiment_id='exp-integration')
token_logger.log(
    step='generation',
    model='claude',
    input_tokens=5000,
    output_tokens=1000,
    cost_usd=0.08,
)
summary = token_logger.get_summary()
print(f'5. Token usage: {summary.total_tokens:,} tokens')

# 6. Track costs
budget = BudgetConfig(daily=1.00)
cost_tracker = CostTracker(budget=budget)
cost_tracker.record(
    experiment_id='exp-integration',
    run_id='run-001',
    estimated=0.10,
    actual=0.08,
)
cost_summary = cost_tracker.get_summary()
print(f'6. Cost tracking: \${cost_summary.total_actual:.2f}')

# Log completion
exp_logger.info(EventType.EXPERIMENT_COMPLETED)

print()
print('Full integration test: PASSED')
"
```

**Expected:** All 6 components initialise and work together, shows "Full integration test: PASSED"

**Pass Criteria:**
- [ ] All logging components work together
- [ ] No errors during integration
- [ ] Data flows correctly between components

---

## Test 10: Unit Tests

**Command:**
```bash
pytest tests/unit/core/logging/ -v --tb=short
```

**Expected:** All 195 logging module tests pass

**Pass Criteria:**
- [ ] All tests pass
- [ ] No unexpected warnings

---

## Results Summary

| Test | Status | Notes |
|------|--------|-------|
| 1. Module Import | ⬜ | |
| 2. Experiment Logger | ⬜ | |
| 3. JSON Lines Logging | ⬜ | |
| 4. Structured Logging | ⬜ | |
| 5. Progress Tracking | ⬜ | |
| 6. Metadata Recording | ⬜ | |
| 7. Token Usage Logging | ⬜ | |
| 8. Cost Tracking | ⬜ | |
| 9. Full Integration | ⬜ | |
| 10. Unit Tests | ⬜ | |

**Overall Result:** ⬜ Pass / ⬜ Fail

---

## Sharing Results

After completing all tests, share:

1. The completed results table above
2. Console output from any failed tests
3. Your environment details:
   - OS:
   - Python version:
   - Any issues encountered

Paste results to Claude for review.
