# Smoke Test: v0.9.0 - Logging & Monitoring (5-10 minutes)

## Overview

This manual test script verifies the v0.9.0 logging and monitoring features work correctly in a real environment.

## Prerequisites

- [ ] Python 3.12+ installed
- [ ] Virtual environment activated: `source .venv/bin/activate`
- [ ] All dependencies installed: `pip install -e ".[all]"`
- [ ] No API keys required (tests use mock responses)

---

## Quick Verification

### 1. Module Import Test

```bash
python -c "from persona.core.logging import ExperimentLogger, StructuredLogger, ProgressTracker; print('✓ Logging modules imported')"
```

**Expected:** `✓ Logging modules imported`

### 2. Experiment Logger (F-073)

```bash
python -c "
from persona.core.logging import ExperimentLogger, EventType, LogLevel

logger = ExperimentLogger(experiment_id='exp-test-001')

# Log various events
logger.info(EventType.EXPERIMENT_STARTED, {'model': 'claude'}, 'Starting experiment')
logger.debug(EventType.DATA_LOADED, {'files': 3})
logger.info(EventType.GENERATION_COMPLETED, {'personas': 5})

# Get events
events = logger.get_events()
print(f'Events logged: {logger.event_count}')
print(f'Info events: {len(logger.get_events(level=LogLevel.INFO))}')
print('✓ ExperimentLogger works')
"
```

**Expected:**
```
Events logged: 3
Info events: 2
✓ ExperimentLogger works
```

### 3. JSON Lines File Logging

```bash
python -c "
import tempfile
from pathlib import Path
from persona.core.logging import ExperimentLogger, EventType, read_log_file

with tempfile.TemporaryDirectory() as tmpdir:
    log_dir = Path(tmpdir) / 'logs'

    with ExperimentLogger(experiment_id='exp-file-test', log_dir=log_dir) as logger:
        logger.info(EventType.EXPERIMENT_STARTED)
        logger.info(EventType.DATA_LOADED, {'count': 5})
        logger.info(EventType.EXPERIMENT_COMPLETED)

    # Read back the log file
    log_file = log_dir / 'experiment.jsonl'
    events = read_log_file(log_file)
    print(f'Events in file: {len(events)}')
    print(f'First event: {events[0].event.value}')
    print('✓ JSON Lines logging works')
"
```

**Expected:** Events read successfully from file.

### 4. Structured Logging (F-074)

```bash
python -c "
from persona.core.logging import StructuredLogger, LogContext, OutputFormat

# Create logger with context
logger = StructuredLogger(output_format=OutputFormat.JSON)
bound = logger.bind(experiment_id='exp-123', run_id='run-456')

# Log events
entry = bound.info('data_loaded', files=3, tokens=5000)
bound.warn('rate_limit_approaching', remaining=5)
bound.error('api_error', status=500)

entries = logger.get_entries()
print(f'Entries logged: {len(entries)}')
print(f'Context preserved: {entries[0].context.experiment_id}')
print('✓ StructuredLogger works')
"
```

**Expected:** Context is preserved across log entries.

### 5. Progress Tracking (F-075)

```bash
python -c "
from persona.core.logging import ProgressTracker

with ProgressTracker(title='Test Progress', quiet=True) as tracker:
    # Add main task
    task_id = tracker.add_task('Loading data', total=100)

    # Update progress
    tracker.update(task_id, advance=30)
    tracker.update(task_id, advance=40)
    tracker.update(task_id, advance=30)

    # Complete
    tracker.complete(task_id)

    task = tracker.get_task(task_id)
    print(f'Task status: {task.status}')
    print(f'Completed: {task.percent:.0f}%')

# Test subtasks
with ProgressTracker(quiet=True) as tracker:
    parent_id = tracker.add_task('Main task', total=100)

    with tracker.subtask(parent_id, 'Subtask 1', total=50) as sub_id:
        tracker.update(sub_id, advance=50)

    parent = tracker.get_task(parent_id)
    print(f'Subtasks: {len(parent.subtasks)}')
    print(f'Subtask status: {parent.subtasks[0].status}')

print('✓ ProgressTracker works')
"
```

**Expected:** Progress tracking and subtasks work.

### 6. Metadata Recording (F-076)

```bash
python -c "
from persona.core.logging import MetadataRecorder, calculate_checksum

# Create recorder
recorder = MetadataRecorder(experiment_id='exp-meta-test', run_id='run-001')

# Record generation
recorder.start()
recorder.set_config(model='claude-sonnet-4', provider='anthropic', persona_count=3)
recorder.add_data_source('interview.csv', tokens=5000, size_bytes=10240)
recorder.set_costs(input_tokens=10000, output_tokens=2000, total_cost_usd=0.15)

# Finish and get metadata
metadata = recorder.finish()

print(f'Experiment: {metadata.experiment_id}')
print(f'Model: {metadata.configuration.model}')
print(f'Duration: {metadata.duration_seconds:.2f}s')
print(f'Cost per persona: \${metadata.costs.cost_per_persona:.4f}')

# Test checksum
checksum = calculate_checksum('test content')
print(f'Checksum prefix: {checksum[:12]}...')

print('✓ MetadataRecorder works')
"
```

**Expected:** Metadata captured correctly.

### 7. Token Usage Logging (F-077)

```bash
python -c "
from persona.core.logging import TokenUsageLogger, TokenBreakdown

logger = TokenUsageLogger(run_id='run-token-test')

# Log multiple calls
logger.log(
    step='generation',
    model='claude-sonnet-4',
    provider='anthropic',
    input_tokens=10000,
    output_tokens=2000,
    cost_usd=0.15,
    breakdown=TokenBreakdown(system=1000, data=7000, instructions=2000),
)

logger.log(
    step='validation',
    model='claude-sonnet-4',
    provider='anthropic',
    input_tokens=5000,
    output_tokens=500,
    cost_usd=0.05,
)

# Get summary
summary = logger.get_summary()
print(f'Total calls: {summary.call_count}')
print(f'Total tokens: {summary.total_tokens:,}')
print(f'Total cost: \${summary.total_cost_usd:.2f}')
print(f'Output/Input ratio: {summary.output_input_ratio:.1%}')

# Export formats
jsonl = logger.to_jsonl()
csv = logger.to_csv()
print(f'JSONL lines: {len(jsonl.strip().split(chr(10)))}')
print(f'CSV lines: {len(csv.strip().split(chr(10)))}')

print('✓ TokenUsageLogger works')
"
```

**Expected:** Token usage tracked and summarised.

### 8. Cost Tracking (F-078)

```bash
python -c "
from persona.core.logging import CostTracker, BudgetConfig

# Create tracker with budget
budget = BudgetConfig(daily=10.00, warn_threshold=0.8, block_threshold=1.0)
tracker = CostTracker(budget=budget)

# Record costs
tracker.record(
    experiment_id='exp-cost-test',
    run_id='run-001',
    estimated=0.50,
    actual=0.48,
    model='claude-sonnet-4',
    input_tokens=10000,
    output_tokens=2000,
)

tracker.record(
    experiment_id='exp-cost-test',
    run_id='run-002',
    estimated=0.50,
    actual=0.52,
    model='claude-sonnet-4',
    input_tokens=10000,
    output_tokens=2500,
)

# Get summary
summary = tracker.get_summary(experiment_id='exp-cost-test')
print(f'Total runs: {summary.total_runs}')
print(f'Total actual: \${summary.total_actual:.2f}')
print(f'Variance: \${summary.total_variance:.4f}')

# Check budget
statuses = tracker.check_budget()
for status in statuses:
    print(f'{status.period}: {status.percent_used:.1f}% used ({status.status})')

print(f'Should block: {tracker.should_block()}')
print(f'Should warn: {tracker.should_warn()}')

print('✓ CostTracker works')
"
```

**Expected:** Cost tracking with budget alerts.

---

## Integration Test

### Full Logging Flow

```bash
python -c "
from persona.core.logging import (
    ExperimentLogger,
    StructuredLogger,
    ProgressTracker,
    MetadataRecorder,
    TokenUsageLogger,
    CostTracker,
    BudgetConfig,
    EventType,
    OutputFormat,
    calculate_checksum,
)

print('=== Full Logging Integration Test ===')
print()

# 1. Setup experiment logging
exp_logger = ExperimentLogger(experiment_id='exp-integration')
exp_logger.info(EventType.EXPERIMENT_STARTED, {'model': 'claude'})
print('1. Experiment logger initialised')

# 2. Setup structured logging
struct_logger = StructuredLogger(output_format=OutputFormat.CONSOLE)
bound = struct_logger.bind(experiment_id='exp-integration')
bound.info('integration_test_started')
print('2. Structured logger with context')

# 3. Track progress
with ProgressTracker(title='Integration Test', quiet=True) as tracker:
    task_id = tracker.add_task('Processing', total=100)
    tracker.update(task_id, completed=100)
    tracker.complete(task_id)
print('3. Progress tracking complete')

# 4. Record metadata
recorder = MetadataRecorder(experiment_id='exp-integration')
recorder.start()
recorder.set_config(model='claude', persona_count=3)
recorder.add_data_source('test.csv', tokens=5000)
recorder.set_costs(input_tokens=5000, output_tokens=1000, total_cost_usd=0.08)
checksum = calculate_checksum('test output')
recorder.add_checksum('output.json', checksum)
metadata = recorder.finish()
print(f'4. Metadata recorded (duration: {metadata.duration_seconds:.2f}s)')

# 5. Log token usage
token_logger = TokenUsageLogger(experiment_id='exp-integration')
token_logger.log(
    step='generation',
    model='claude',
    input_tokens=5000,
    output_tokens=1000,
    cost_usd=0.08,
)
summary = token_logger.get_summary()
print(f'5. Token usage: {summary.total_tokens:,} tokens')

# 6. Track costs
budget = BudgetConfig(daily=1.00)
cost_tracker = CostTracker(budget=budget)
cost_tracker.record(
    experiment_id='exp-integration',
    run_id='run-001',
    estimated=0.10,
    actual=0.08,
)
cost_summary = cost_tracker.get_summary()
print(f'6. Cost tracking: \${cost_summary.total_actual:.2f}')

# Log completion
exp_logger.info(EventType.EXPERIMENT_COMPLETED)

print()
print('✓ Full integration test passed')
"
```

**Expected:** All logging components work together.

---

## Unit Test Verification

```bash
pytest tests/unit/core/logging/ -v --tb=short
```

**Expected:** All 195 tests pass.

---

## Report

- **Version:** v0.9.0
- **Date:** ___
- **Result:** PASS / FAIL
- **Notes:** ___

### Quick Checklist

- [ ] Module imports work
- [ ] ExperimentLogger works
- [ ] JSON Lines file logging works
- [ ] StructuredLogger works
- [ ] ProgressTracker works
- [ ] MetadataRecorder works
- [ ] TokenUsageLogger works
- [ ] CostTracker works
- [ ] Integration test passes
- [ ] All unit tests pass

---

## Issues Found

_List any issues discovered during testing:_

1. _Issue description_
   - Steps to reproduce
   - Console output
