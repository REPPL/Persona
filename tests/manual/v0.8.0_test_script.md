# Smoke Test: v0.8.0 - Multi-Model & Quality Assurance (5-10 minutes)

## Overview

This manual test script verifies the v0.8.0 multi-model generation features work correctly in a real environment.

## Prerequisites

- [ ] Python 3.12+ installed
- [ ] Virtual environment activated: `source .venv/bin/activate`
- [ ] All dependencies installed: `pip install -e ".[all]"`
- [ ] No API keys required (tests use mock responses)

---

## Quick Verification

### 1. Module Import Test

```bash
python -c "from persona.core.multimodel import MultiModelGenerator, ModelSpec; print('✓ MultiModelGenerator imported')"
```

**Expected:** `✓ MultiModelGenerator imported`

### 2. Model Spec Parsing

```bash
python -c "
from persona.core.multimodel import ModelSpec
spec = ModelSpec.parse('anthropic:claude-sonnet-4')
print(f'Provider: {spec.provider}')
print(f'Model: {spec.model}')
print('✓ ModelSpec parsing works')
"
```

**Expected:**
```
Provider: anthropic
Model: claude-sonnet-4
✓ ModelSpec parsing works
```

### 3. Execution Strategies

```bash
python -c "
from persona.core.multimodel import ParallelStrategy, SequentialStrategy, ConsensusStrategy
from persona.core.multimodel import ModelSpec
models = [ModelSpec('anthropic', 'claude-sonnet-4'), ModelSpec('openai', 'gpt-4o')]

# Parallel
parallel = ParallelStrategy()
result = parallel.execute('Test data', models, count=2, temperature=0.7, max_tokens=4096)
print(f'Parallel mode: {result.execution_mode}, outputs: {len(result.model_outputs)}')

# Sequential
sequential = SequentialStrategy()
result = sequential.execute('Test data', models, count=2, temperature=0.7, max_tokens=4096)
print(f'Sequential mode: {result.execution_mode}, outputs: {len(result.model_outputs)}')

# Consensus
consensus = ConsensusStrategy()
result = consensus.execute('Test data', models, count=3, temperature=0.7, max_tokens=4096)
print(f'Consensus mode: {result.execution_mode}, outputs: {len(result.model_outputs)}')

print('✓ All execution strategies work')
"
```

**Expected:**
```
Parallel mode: parallel, outputs: 2
Sequential mode: sequential, outputs: 2
Consensus mode: consensus, outputs: 2
✓ All execution strategies work
```

### 4. Coverage Analysis

```bash
python -c "
from persona.core.multimodel import CoverageAnalyser
analyser = CoverageAnalyser()
personas = [
    {'id': '1', 'role': 'Developer', 'goals': ['improve onboarding']},
    {'id': '2', 'role': 'Designer', 'goals': ['better UX']},
]
analysis = analyser.analyse(personas, themes=['onboarding', 'UX', 'security'])
print(f'Themes analysed: {len(analysis.theme_coverage)}')
print(f'Gaps found: {len(analysis.gaps)}')
print(f'Score: {analysis.overall_score:.0f}%')
print('✓ Coverage analysis works')
"
```

**Expected:** Coverage analysis runs without error.

### 5. Confidence Scoring

```bash
python -c "
from persona.core.multimodel import ConfidenceScorer
scorer = ConfidenceScorer()
persona = {
    'id': '1',
    'role': 'Developer',
    'goals': ['code quality'],
    'frustrations': ['slow builds'],
}
source_data = {
    'interview.md': 'The developer emphasized code quality multiple times. Code quality is key.',
}
result = scorer.score_persona(persona, source_data)
print(f'Persona: {result.persona_id}')
print(f'Overall: {result.overall_confidence.value}')
print(f'Score: {result.confidence_score:.0%}')
print('✓ Confidence scoring works')
"
```

**Expected:** Confidence scoring runs without error.

### 6. Consolidation Mapping

```bash
python -c "
from persona.core.multimodel import ConsolidationMapper
mapper = ConsolidationMapper()
personas = [
    {'id': '1', 'role': 'Developer', 'goals': ['A', 'B'], 'frustrations': ['X']},
    {'id': '2', 'role': 'Developer', 'goals': ['A', 'B'], 'frustrations': ['X']},
    {'id': '3', 'role': 'Designer', 'goals': ['C'], 'frustrations': ['Y']},
]
result = mapper.consolidate(personas)
print(f'Clusters: {len(result.clusters)}')
print(f'Unique personas: {len(result.unique_personas)}')
print(f'After consolidation: {result.consolidated_count}')
print('✓ Consolidation mapping works')
"
```

**Expected:** Consolidation runs without error.

### 7. Cost Estimation

```bash
python -c "
from persona.core.multimodel import MultiModelCostEstimator, ModelSpec
estimator = MultiModelCostEstimator()
models = [
    ModelSpec('anthropic', 'claude-sonnet-4-20250514'),
    ModelSpec('openai', 'gpt-4o'),
]
breakdown = estimator.estimate(models, input_tokens=50000, persona_count=3, mode='parallel')
print(f'Models: {len(breakdown.model_costs)}')
print(f'Subtotal: \${breakdown.subtotal:.4f}')
print(f'Total: \${breakdown.total_cost:.4f}')
print('✓ Cost estimation works')
"
```

**Expected:** Cost breakdown displays.

### 8. Model Capabilities

```bash
python -c "
from persona.core.multimodel import CapabilityChecker, CapabilityQuery
checker = CapabilityChecker()

# Get capabilities
caps = checker.get_capabilities('anthropic', 'claude-sonnet-4-20250514')
print(f'Model: {caps.model}')
print(f'Context: {caps.context_window:,} tokens')
print(f'Vision: {caps.vision}')

# Query models
query = CapabilityQuery(requires_vision=True, pricing_tier='budget')
matches = checker.find_models(query)
print(f'Budget models with vision: {len(matches)}')

print('✓ Capability checking works')
"
```

**Expected:** Capability information displays.

---

## Integration Test

### Full Multi-Model Generation Flow

```bash
python -c "
from persona.core.multimodel import (
    MultiModelGenerator,
    ModelSpec,
    CoverageAnalyser,
    ConfidenceScorer,
    ConsolidationMapper,
    MultiModelCostEstimator,
)

# 1. Generate with multiple models
generator = MultiModelGenerator()
models = [
    ModelSpec('anthropic', 'claude-sonnet-4'),
    ModelSpec('openai', 'gpt-4o'),
]
result = generator.generate(
    data='User research about software developers and their workflows',
    models=models,
    count=3,
    mode='parallel',
)
print(f'Generated {len(result.all_personas)} personas from {len(result.model_outputs)} models')

# 2. Analyse coverage
analyser = CoverageAnalyser()
analysis = analyser.analyse(result.all_personas, themes=['workflow', 'tools', 'challenges'])
print(f'Coverage score: {analysis.overall_score:.0f}%')

# 3. Score confidence
scorer = ConfidenceScorer()
confidences = scorer.score_personas(result.all_personas)
avg_score = sum(c.confidence_score for c in confidences) / len(confidences)
print(f'Average confidence: {avg_score:.0%}')

# 4. Consolidate
mapper = ConsolidationMapper()
consolidation = mapper.consolidate(result.all_personas)
print(f'Consolidated to {consolidation.consolidated_count} unique personas')

# 5. Estimate cost
estimator = MultiModelCostEstimator()
cost = estimator.estimate(models, input_tokens=10000, persona_count=3, mode='parallel')
print(f'Estimated cost: \${cost.total_cost:.4f}')

print()
print('✓ Full integration test passed')
"
```

**Expected:** All steps complete without error.

---

## Unit Test Verification

```bash
pytest tests/unit/core/multimodel/ -v --tb=short
```

**Expected:** All 139 tests pass.

---

## Report

- **Version:** v0.8.0
- **Date:** ___
- **Result:** PASS / FAIL
- **Notes:** ___

### Quick Checklist

- [ ] Module imports work
- [ ] Model spec parsing works
- [ ] Parallel strategy works
- [ ] Sequential strategy works
- [ ] Consensus strategy works
- [ ] Coverage analysis works
- [ ] Confidence scoring works
- [ ] Consolidation mapping works
- [ ] Cost estimation works
- [ ] Model capabilities work
- [ ] Integration test passes
- [ ] All unit tests pass

---

## Issues Found

_List any issues discovered during testing:_

1. _Issue description_
   - Steps to reproduce
   - Console output
