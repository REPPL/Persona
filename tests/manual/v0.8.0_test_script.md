# Manual Test Script: v0.8.0 - Multi-Model & Quality Assurance

This script verifies all v0.8.0 functionality. Execute tests in order and record results.

## Prerequisites

- Python 3.12+
- Git checkout of v0.8.0 tag
- No API keys required (tests use mock responses)

## Installation

```bash
# 1. Checkout the version
git checkout v0.8.0

# 2. Create fresh virtual environment
python3.12 -m venv .venv-test
source .venv-test/bin/activate  # On Windows: .venv-test\Scripts\activate

# 3. Install Persona with all dependencies
pip install -e ".[all]"

# 4. Verify installation
python -c "import persona; print(f'Persona v{persona.__version__}')"
```

**Expected:** Shows Persona v0.8.0

---

## Test 1: Module Import Test

**Command:**
```bash
python -c "from persona.core.multimodel import MultiModelGenerator, ModelSpec; print('MultiModelGenerator imported')"
```

**Expected:** Prints "MultiModelGenerator imported"

**Pass Criteria:**
- [ ] MultiModelGenerator imports successfully

---

## Test 2: Model Spec Parsing

**Command:**
```bash
python -c "
from persona.core.multimodel import ModelSpec
spec = ModelSpec.parse('anthropic:claude-sonnet-4')
print(f'Provider: {spec.provider}')
print(f'Model: {spec.model}')
print('ModelSpec parsing works')
"
```

**Expected:** Provider: anthropic, Model: claude-sonnet-4, then "ModelSpec parsing works"

**Pass Criteria:**
- [ ] Provider parsed correctly
- [ ] Model parsed correctly

---

## Test 3: Execution Strategies

**Command:**
```bash
python -c "
from persona.core.multimodel import ParallelStrategy, SequentialStrategy, ConsensusStrategy
from persona.core.multimodel import ModelSpec
models = [ModelSpec('anthropic', 'claude-sonnet-4'), ModelSpec('openai', 'gpt-4o')]

# Parallel
parallel = ParallelStrategy()
result = parallel.execute('Test data', models, count=2, temperature=0.7, max_tokens=4096)
print(f'Parallel mode: {result.execution_mode}, outputs: {len(result.model_outputs)}')

# Sequential
sequential = SequentialStrategy()
result = sequential.execute('Test data', models, count=2, temperature=0.7, max_tokens=4096)
print(f'Sequential mode: {result.execution_mode}, outputs: {len(result.model_outputs)}')

# Consensus
consensus = ConsensusStrategy()
result = consensus.execute('Test data', models, count=3, temperature=0.7, max_tokens=4096)
print(f'Consensus mode: {result.execution_mode}, outputs: {len(result.model_outputs)}')

print('All execution strategies work')
"
```

**Expected:** Parallel mode: parallel, outputs: 2; Sequential mode: sequential, outputs: 2; Consensus mode: consensus, outputs: 2; then "All execution strategies work"

**Pass Criteria:**
- [ ] Parallel strategy works
- [ ] Sequential strategy works
- [ ] Consensus strategy works

---

## Test 4: Coverage Analysis

**Command:**
```bash
python -c "
from persona.core.multimodel import CoverageAnalyser
analyser = CoverageAnalyser()
personas = [
    {'id': '1', 'role': 'Developer', 'goals': ['improve onboarding']},
    {'id': '2', 'role': 'Designer', 'goals': ['better UX']},
]
analysis = analyser.analyse(personas, themes=['onboarding', 'UX', 'security'])
print(f'Themes analysed: {len(analysis.theme_coverage)}')
print(f'Gaps found: {len(analysis.gaps)}')
print(f'Score: {analysis.overall_score:.0f}%')
print('Coverage analysis works')
"
```

**Expected:** Coverage analysis runs without error, shows themes analysed, gaps found, and score

**Pass Criteria:**
- [ ] Themes analysed
- [ ] Gaps identified
- [ ] Score calculated

---

## Test 5: Confidence Scoring

**Command:**
```bash
python -c "
from persona.core.multimodel import ConfidenceScorer
scorer = ConfidenceScorer()
persona = {
    'id': '1',
    'role': 'Developer',
    'goals': ['code quality'],
    'frustrations': ['slow builds'],
}
source_data = {
    'interview.md': 'The developer emphasized code quality multiple times. Code quality is key.',
}
result = scorer.score_persona(persona, source_data)
print(f'Persona: {result.persona_id}')
print(f'Overall: {result.overall_confidence.value}')
print(f'Score: {result.confidence_score:.0%}')
print('Confidence scoring works')
"
```

**Expected:** Confidence scoring runs without error, shows persona ID, overall confidence level, and score percentage

**Pass Criteria:**
- [ ] Persona ID shown
- [ ] Confidence level calculated
- [ ] Score percentage displayed

---

## Test 6: Consolidation Mapping

**Command:**
```bash
python -c "
from persona.core.multimodel import ConsolidationMapper
mapper = ConsolidationMapper()
personas = [
    {'id': '1', 'role': 'Developer', 'goals': ['A', 'B'], 'frustrations': ['X']},
    {'id': '2', 'role': 'Developer', 'goals': ['A', 'B'], 'frustrations': ['X']},
    {'id': '3', 'role': 'Designer', 'goals': ['C'], 'frustrations': ['Y']},
]
result = mapper.consolidate(personas)
print(f'Clusters: {len(result.clusters)}')
print(f'Unique personas: {len(result.unique_personas)}')
print(f'After consolidation: {result.consolidated_count}')
print('Consolidation mapping works')
"
```

**Expected:** Consolidation runs without error, shows cluster count, unique personas, and consolidated count

**Pass Criteria:**
- [ ] Clusters identified
- [ ] Unique personas found
- [ ] Consolidation count shown

---

## Test 7: Cost Estimation

**Command:**
```bash
python -c "
from persona.core.multimodel import MultiModelCostEstimator, ModelSpec
estimator = MultiModelCostEstimator()
models = [
    ModelSpec('anthropic', 'claude-sonnet-4-20250514'),
    ModelSpec('openai', 'gpt-4o'),
]
breakdown = estimator.estimate(models, input_tokens=50000, persona_count=3, mode='parallel')
print(f'Models: {len(breakdown.model_costs)}')
print(f'Subtotal: \${breakdown.subtotal:.4f}')
print(f'Total: \${breakdown.total_cost:.4f}')
print('Cost estimation works')
"
```

**Expected:** Cost breakdown displays with models count, subtotal, and total cost

**Pass Criteria:**
- [ ] Model costs calculated
- [ ] Subtotal shown
- [ ] Total cost displayed

---

## Test 8: Model Capabilities

**Command:**
```bash
python -c "
from persona.core.multimodel import CapabilityChecker, CapabilityQuery
checker = CapabilityChecker()

# Get capabilities
caps = checker.get_capabilities('anthropic', 'claude-sonnet-4-20250514')
print(f'Model: {caps.model}')
print(f'Context: {caps.context_window:,} tokens')
print(f'Vision: {caps.vision}')

# Query models
query = CapabilityQuery(requires_vision=True, pricing_tier='budget')
matches = checker.find_models(query)
print(f'Budget models with vision: {len(matches)}')

print('Capability checking works')
"
```

**Expected:** Capability information displays with model name, context window, vision support, and query results

**Pass Criteria:**
- [ ] Model capabilities retrieved
- [ ] Context window shown
- [ ] Vision support indicated
- [ ] Model query works

---

## Test 9: Full Integration Test

**Command:**
```bash
python -c "
from persona.core.multimodel import (
    MultiModelGenerator,
    ModelSpec,
    CoverageAnalyser,
    ConfidenceScorer,
    ConsolidationMapper,
    MultiModelCostEstimator,
)

# 1. Generate with multiple models
generator = MultiModelGenerator()
models = [
    ModelSpec('anthropic', 'claude-sonnet-4'),
    ModelSpec('openai', 'gpt-4o'),
]
result = generator.generate(
    data='User research about software developers and their workflows',
    models=models,
    count=3,
    mode='parallel',
)
print(f'Generated {len(result.all_personas)} personas from {len(result.model_outputs)} models')

# 2. Analyse coverage
analyser = CoverageAnalyser()
analysis = analyser.analyse(result.all_personas, themes=['workflow', 'tools', 'challenges'])
print(f'Coverage score: {analysis.overall_score:.0f}%')

# 3. Score confidence
scorer = ConfidenceScorer()
confidences = scorer.score_personas(result.all_personas)
avg_score = sum(c.confidence_score for c in confidences) / len(confidences)
print(f'Average confidence: {avg_score:.0%}')

# 4. Consolidate
mapper = ConsolidationMapper()
consolidation = mapper.consolidate(result.all_personas)
print(f'Consolidated to {consolidation.consolidated_count} unique personas')

# 5. Estimate cost
estimator = MultiModelCostEstimator()
cost = estimator.estimate(models, input_tokens=10000, persona_count=3, mode='parallel')
print(f'Estimated cost: \${cost.total_cost:.4f}')

print()
print('Full integration test passed')
"
```

**Expected:** All steps complete without error, shows persona count, coverage score, average confidence, consolidated count, and estimated cost

**Pass Criteria:**
- [ ] Multi-model generation works
- [ ] Coverage analysis completes
- [ ] Confidence scoring completes
- [ ] Consolidation completes
- [ ] Cost estimation completes

---

## Test 10: Run Automated Tests

**Command:**
```bash
pytest tests/unit/core/multimodel/ -v --tb=short
```

**Expected:** All 139 tests pass

**Pass Criteria:**
- [ ] All tests pass

---

## Results Summary

| Test | Status | Notes |
|------|--------|-------|
| 1. Module Import | ⬜ | |
| 2. Model Spec Parsing | ⬜ | |
| 3. Execution Strategies | ⬜ | |
| 4. Coverage Analysis | ⬜ | |
| 5. Confidence Scoring | ⬜ | |
| 6. Consolidation Mapping | ⬜ | |
| 7. Cost Estimation | ⬜ | |
| 8. Model Capabilities | ⬜ | |
| 9. Full Integration | ⬜ | |
| 10. Automated Tests | ⬜ | |

**Overall Result:** ⬜ Pass / ⬜ Fail

---

## Sharing Results

After completing all tests, share:

1. The completed results table above
2. Console output from any failed tests
3. Your environment details:
   - OS:
   - Python version:
   - Any issues encountered

Paste results to Claude for review.
